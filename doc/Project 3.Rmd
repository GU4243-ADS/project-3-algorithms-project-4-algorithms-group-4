---
title: "Collaborative Filtering"
author: "Group 4"
date: "April 9, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
setwd("/Users/Nicole/Documents/GitHub/project-3-algorithms-project-4-algorithms-group-4/doc")
#setwd("/Users/admin/Desktop/Columbia/Spring 2018/Applied DS/GitHub/project-3-algorithms-project-4-algorithms-group-4/doc")

load(file = "../data/MS_UI.RData")
load(file = "../data/movie_UI.RData")
```

```{r Matrix Calculations}
visit_nums <- rowSums(MS_UI, na.rm = TRUE)

# table(visit_nums)
# mean(visit_nums)
# median(visit_nums)

total_ratings <- rowSums(movie_UI, na.rm = TRUE)

# table(total_ratings)
# mean(total_ratings)
# median(total_ratings)
```


```{r Simularity Weights of the Users}
# Pearson correlations
load(file = "../data/MS_sim.RData")
load(file = "../data/movie_sim.RData")

# Spearman correlations

# Vector similarity (Cosine)

# Entropy-based

# Mean-square difference

# SimRank

```

```{r Calculating Predictions for Users}
# From Pearson Simularity Weights
load(file ="../data/MS_pred.RData")
load(file = "../data/movie_pred.RData")
```

Group 4:
- After calculating similarity weights, we select which other users' data are used in computing the predictions (currently, all of them).
- There is evidence that selecting a subset of users improves accuracy. 
- Moreover, when there are millions of users, using them all is infeasible. 

Explore: Will prediction accuracy improve if we select the best neighbors of the active user to use in calculating predictions?

#Model-based Algorithm
```{r Model-Based Algorithm}

mov.train <- movie_UI # to clarify that this is training data

EM_func <= function(data, C = 1, tau = 0.01, ITER = 100){ 
  # function arguments include:
      #data: user-item matrix
      #C: number of clusters
      #tau: threshold for termination when values converge
      #iter: maximum number of iterations
 # function output = ???
  
  #### Step 1 - Initial Conditions 
  iter <- 1
  data <- mov.train[1:500, ] # subset for speed (until finished)
  C <- 12 # Half of the movie genres according to IMDB
  
  k = 6 # number of ratings
  users <- rownames(mov.train)
  items <- colnames(mov.train) # used items as a generic for both movies and vroots
  
  mu <- rep(1/C, C) 
  
  gamma <- array(0, dim = c(length(items), C, k)) # dims = items(movies), clusters, ratings
  
  cluster.probs <- matrix(NA, nrow = length(users), ncol = C) # cluster assignment matrix 

  
  #### Step 2 - Expectation 
  while(is.conv > tau | iter < ITER){
    max.lik <- log(gamma)
    phi <- exp(data %*% t(max.lik)) * mu
    cluster.probs <- matrix(rep(1/rowSums(phi), C), length(users), C) * phi
  
    }
  #### Step 3 - Maximization 
  #set.seed(123)
  #test <- matrix(sample(20, 24, replace = TRUE), nrow = 4, ncol = 6)
  # use the %in% func to create the matrix I want to operate on
 
  ## Mu 
  mu <- apply(cluster.probs, 2, sum)/length(users) # dims = 1 x clusters
  
  ## Gamma
  
#gamma <- function(k=1) {
  #k = 1
  #C = 6
  #cluster.probs <- matrix((1/6), nrow = length(users), ncol = C) # dims = users, clusters
  
  indicator.numer <- array(0, dim = c(length(users),length(items), k))
  indicator.denom <- matrix(0, nrow = length(users), ncol = length(items))
    for (i in 1:k) #can this be done with an apply appoach? yes, apply and margin = 3? but how to use apply with multiple funcs and subsetting, maybe define a func like below?
    {
      indicator.numer[,,i][which(mov.train == i)] <- 1 
    }
  indicator.numer <- aperm(indicator.numer, c(2,1,3)) 
  # would transposing each sheet in the for loop be faster?
   
  numer <- apply(indicator.numer, 3, function(x) x%*%cluster.probs)
  numerator <- array(numer, c(length(items), C, k)) # dims = movies, clusters, ratings
  # maybe faster to transpose cluster.probs rather than aperm indicator.nomin
  #nominator <- indicator.nomin %*% cluster.probs # dims = movies, clusters, ratings 
  
  indicator.denom[which(!is.na(mov.train))] <- 1
  indicator.denom <- t(indicator.denom) # dims = movies, users  
  #denominator is common across ratings k, but not across clusters
  demoninator <- indicator.denom %*% cluster.probs # dims = movies, clusters
  #demoninator <- apply(indicator.denom, 3, function(x) x%*%cluster.probs)
  
  #result <- nominator/demoninator # dims = movies, clusters, ratings
  result <- apply(numerator, 3, function(x) x/demoninator)
  result <- array(result, c(length(items), C, k)) # dims = movies, clusters, ratings
  str(result)
#}
#system.time(gamma())
  
  # !! check gamma is correct by making sure that sum over all k's of gamma_j,c = 1
  



  #Other approaches: 
  #  (1)j_k in users for all j
  #  (2) lapply concatatnating gamma_{j,1:k} cols
  #  (3) 3d by 3d matrix mult (not really possible in R)
  
  # convergence using l-2 norm 
  is.conv <- norm(mu_hat - mu_pre)
  print(is.conv)
  print(iter)
  
  mu_pre <- mu_hat
  iter = iter + 1
  }
  
  return(list("clusters" = cluster.probs, "gamma" = gamma, "mu" = mu))
}


```


Model-based recommendation intermediate steps:
-(E-step) Write code to give cluster assignment probabilities to user, a matrix where cols are clusters and rows are users. output = assignment probability matrix.

-(M- Step) Write code to re-estimate mu and gamma vectors from each users cluster assignment probabilities. There are $k*C*j$ gamma values which can be put into a 3d array or a series of matrices. Output = recalculated mu and gamma (matrix).

- Write code to (a) iterate over the the two steps, (b) to set when to stop iteration(consider iteration back stop number), (c) set up initial $\mu_c$ and $\gamma_{j,C,k}$ and other values, and (d) make "hard" cluster assignments from the "soft" assignments of the final iteration of EM. Also, do we do uniform dist. initial values or do we do random initial values and choose best result (more complicated.. we can test to see if it even makes a difference)

-Write code to calculate expected active user rating from EM parameters. Output = function with input of data and # of classes and output of predicted rating on movie m for active user.

-Write CV code to determine best test error on predictions at different numbers of classes for the EM/predictor model. Include code to calculate test error (shared with memory-based team). Q: do we evaluate CV based on log likelihood of the EM model or the test error of the prediction model? Output = optimal number of classes. 

-Re calibrate for microsoft data.


Notes on EM: 
- Use cross-validation to find optimal number of clusters (if computationally taxing, then use a k-fold CV where k = 1)
- Terminate when if iter = 10000 or if $\sum_{c = 1}^C |{\hat{\mu}_c^{t+1} - \hat{\mu}_c^{t}}|^2$ where |x| is the l-2 norm. 
- Can do random initializations and find log-likelihood but not necessary

Notes to make predictions from EM: 
- After the termination of the EM algorithm, give a hard assignment of clusters (EM algorithm only gives soft assignment so use the highest probability that a user falls within a specific cluster to create hard assignment)
- For each movie-cluster pair, you have a multinomial distribution (this is $\gamma$); use this probability distribution to calculate the rating that each user \textit{a} gave to movie \textit{m} within each cluster \textit{c}. 
- All of the users within each cluster have the same rating probability. 
