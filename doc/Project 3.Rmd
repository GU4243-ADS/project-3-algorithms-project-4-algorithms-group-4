---
title: "Collaborative Filtering"
author: "Group 4"
date: "April 9, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
setwd("/Users/Nicole/Documents/GitHub/project-3-algorithms-project-4-algorithms-group-4/doc")
#setwd("/Users/admin/Desktop/Columbia/Spring 2018/Applied DS/GitHub/project-3-algorithms-project-4-algorithms-group-4/doc")

load(file = "../data/MS_UI.RData")
load(file = "../data/movie_UI.RData")
```

```{r Matrix Calculations}
visit_nums <- rowSums(MS_UI, na.rm = TRUE)

# table(visit_nums)
# mean(visit_nums)
# median(visit_nums)

total_ratings <- rowSums(movie_UI, na.rm = TRUE)

# table(total_ratings)
# mean(total_ratings)
# median(total_ratings)
```


```{r Simularity Weights of the Users}
# Pearson correlations
load(file = "../data/MS_sim.RData")
load(file = "../data/movie_sim.RData")

# Spearman correlations

# Vector similarity (Cosine)

# Entropy-based

# Mean-square difference

# SimRank

```

```{r Calculating Predictions for Users}
# From Pearson Simularity Weights
load(file ="../data/MS_pred.RData")
load(file = "../data/movie_pred.RData")
```

Group 4:
- After calculating similarity weights, we select which other users' data are used in computing the predictions (currently, all of them).
- There is evidence that selecting a subset of users improves accuracy. 
- Moreover, when there are millions of users, using them all is infeasible. 

Explore: Will prediction accuracy improve if we select the best neighbors of the active user to use in calculating predictions?

# Model-based Algorithm
```{r Model-Based Algorithm}
library(matrixStats)

mov.train <- movie_UI # to clarify that this is training data, Note: the unrated user-movie combos are NA's



EM_func <= function(data, C = 5, tau = 0.01, ITER = 100){ 
  
  #### Step 1 - Initial Conditions 
  set.seed(123)
  #data <- mov.train[1:500, ] # subset for speed (until finished)
  data[which(is.na(data))] <- 0 # Note: length(which(rowSums(data) == 0)) is 0 which means ther is extension on at least 1 dimension for each user, i.e. every user rated at least 1 movie
  #C <- 12 # Half of the movie genres according to IMDB
  #k = 6 # number of ratings
  users <- rownames(data)
  items <- colnames(data) # used items as a generic for both movies and vroots
  mu <- rep(1/C, C) 
  gamma <- array(1/k, dim = c(length(items), C, k)) # dims = items(movies), clusters, ratings
  aic <- matrix(1/C, nrow = length(users), ncol = C) # cluster assignment/probability matrix, dims = users, clusters
  gamma.norm <- gamma/mean(gamma)
  
  iter <- 1
  while(is.conv > tau | iter < ITER) {
   
  #### Step 2 - Expectation
    
    e.step1 <- log(mu) + log(gamma) # numerator of aic using log probabilities
    e.step2 <- logSumExp(mu * gamma) # normalizing 
    aic <- e.step1 - e.step2 
    
    
    # in the numerator we multiply all gamma[c,j,k] for a given class c, all movies j that have been rated by the active user, k being the rating that the active user actually gave to this movie ? And in the denominator sum all such products for all classes ?
    
    # logSumExp(vector of probabilities?) -- the step where a_ic = max c
    # The usual way to handle this is to work with log-probabilities instead of probabilities. This allows us to handle very small numbers without problem. When applying logs to the formula, the only difficulty is then dealing with the denominator that contains a sum.
  
  #### Step 3 - Maximization 
 
  ## Estimate Mu 
  mu <- apply(aic, 2, sum)/length(users) # dims = 1 x clusters
  
  ## Estimate Gamma
  
  gamma <- function(k=1) { # this is func is just diagnostic for the moment
  
  indicator.numer <- array(0, dim = c(length(users),length(items), k))
  indicator.denom <- matrix(0, nrow = length(users), ncol = length(items))
    
  for (i in 1:k) 
    # can this be done with an apply appoach? yes, apply and margin = 3? but how to use apply with multiple funcs and subsetting, maybe define a func like below?
    {
      indicator.numer[,,i][which(data == i)] <- 1 # indicator array to identify each instance in which user i rated movie j with rating k
    }
  indicator.numer <- aperm(indicator.numer, c(2,1,3)) # transposing so that the matrix is conformable to aic. new dims = (movies/items, users, ratings)
  # would transposing each sheet in the for loop be faster?
  # maybe faster to transpose aic rather than aperm indicator.nomin
   
  numer <- apply(indicator.numer, 3, function(x) x%*%aic) # each element of resulting array represents the sum of the class C aic wieghts of every user who rated that jth movie with that rating k. apply() outputs matrix with weird dims = (length(seq_along(\the result of the func on each sheet\)), number of sheets in third margin)
  numerator <- array(numer, c(length(items), C, k)) # puts data in the correct shape. dims = (movies, clusters, ratings) 
  
  #denominator <- indicator.denom %*% aic # dims = movies, clusters, ratings 
  
  indicator.denom[which(data != 0)] <- 1
  indicator.denom <- t(indicator.denom) # dims = movies, users  
  #denominator is common across ratings k, but not across clusters
  demoninator <- indicator.denom %*% aic # dims = movies, clusters
  #demoninator <- apply(indicator.denom, 3, function(x) x%*%aic)
  
  #result <- nominator/demoninator # dims = movies, clusters, ratings
  result <- apply(numerator, 3, function(x) x/demoninator)
  result <- array(result, c(length(items), C, k)) # dims = movies, clusters, ratings
  result
} # end of the diagnostic func
system.time(new <- gamma(k=3))
  
  # !! check gamma is correct by making sure that sum over all k's of gamma_j,c = 1
  
  for (i in 1:k){}
    gamma.numer[, , ] <- t(aic) %*% (movie == k)
    gamma.denom[, , ] <- colSums(gamma.numer)

    gamma.parameter <- gamma.numer/gamma.denom
  }



  #Other approaches: 
  #  (1)j_k in users for all j
  #  (2) lapply concatatnating gamma_{j,1:k} cols
  #  (3) 3d by 3d matrix mult (not really possible in R)
  
  # convergence using l-2 norm 
  is.conv <- norm(aic - aic_pre, type = "O") # measure of the change in cluster assignments

  aic_pre <- aic
  iter = iter + 1
  }
  
  return(list("clusters" = aic, "gamma" = gamma, "mu" = mu))
}

```

https://d1b10bmlvqabco.cloudfront.net/attach/ja4fumjiqyo475/isgbsiqhYqF/jfmthrwhv767/HW5_sol.pdf


Try simplifying the sum and exponential to see that it is the same equation as in the last two slides. It is easy to run into underflow problem when dealing with these probabilities that are very small. You can see the answers in @104 to see how to deal with it (the usual strategy is to use log-probabilities instead).


Model-based recommendation intermediate steps:
-(E-step) Write code to give cluster assignment probabilities to user, a matrix where cols are clusters and rows are users. output = assignment probability matrix.

-(M- Step) Write code to re-estimate mu and gamma vectors from each users cluster assignment probabilities. There are $k*C*j$ gamma values which can be put into a 3d array or a series of matrices. Output = recalculated mu and gamma (matrix).

- Write code to (a) iterate over the the two steps, (b) to set when to stop iteration(consider iteration back stop number), (c) set up initial $\mu_c$ and $\gamma_{j,C,k}$ and other values, and (d) make "hard" cluster assignments from the "soft" assignments of the final iteration of EM. Also, do we do uniform dist. initial values or do we do random initial values and choose best result (more complicated.. we can test to see if it even makes a difference)

-Write code to calculate expected active user rating from EM parameters. Output = function with input of data and # of classes and output of predicted rating on movie m for active user.

-Write CV code to determine best test error on predictions at different numbers of classes for the EM/predictor model. Include code to calculate test error (shared with memory-based team). Q: do we evaluate CV based on log likelihood of the EM model or the test error of the prediction model? Output = optimal number of classes. 

-Re calibrate for microsoft data.


Notes on EM: 
- Use cross-validation to find optimal number of clusters (if computationally taxing, then use a k-fold CV where k = 1)
- Terminate when if iter = 10000 or if $\sum_{c = 1}^C |{\hat{\mu}_c^{t+1} - \hat{\mu}_c^{t}}|^2$ where |x| is the l-2 norm. 
- Can do random initializations and find log-likelihood but not necessary

Notes to make predictions from EM: 
- After the termination of the EM algorithm, give a hard assignment of clusters (EM algorithm only gives soft assignment so use the highest probability that a user falls within a specific cluster to create hard assignment)
- For each movie-cluster pair, you have a multinomial distribution (this is $\gamma$); use this probability distribution to calculate the rating that each user \textit{a} gave to movie \textit{m} within each cluster \textit{c}. 
- All of the users within each cluster have the same rating probability.

```{r}
# Predictions
hard.cluster <- apply(aic, 1, which.max) # hard cluster assignments
```

