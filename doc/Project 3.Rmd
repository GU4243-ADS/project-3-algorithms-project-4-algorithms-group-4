---
title: "Collaborative Filtering"
author: "Group 4"
date: "April 9, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
setwd("/Users/Nicole/Documents/GitHub/project-3-algorithms-project-4-algorithms-group-4/doc")

load(file = "../data/MS_UI.RData")
load(file = "../data/movie_UI.RData")
```

```{r Matrix Calculations}
visit_nums <- rowSums(MS_UI, na.rm = TRUE)

# table(visit_nums)
# mean(visit_nums)
# median(visit_nums)

total_ratings <- rowSums(movie_UI, na.rm = TRUE)

# table(total_ratings)
# mean(total_ratings)
# median(total_ratings)
```


```{r Simularity Weights of the Users}
# Pearson correlations
load(file = "../data/MS_sim.RData")
load(file = "../data/movie_sim.RData")

# Spearman correlations

# Vector similarity (Cosine)

# Entropy-based

# Mean-square difference

# SimRank

```

```{r Calculating Predictions for Users}
# From Pearson Simularity Weights
load(file ="../data/MS_pred.RData")
load(file = "../data/movie_pred.RData")
```

Group 4:
- After calculating similarity weights, we select which other users' data are used in computing the predictions (currently, all of them).
- There is evidence that selecting a subset of users improves accuracy. 
- Moreover, when there are millions of users, using them all is infeasible. 

Explore: Will prediction accuracy improve if we select the best neighbors of the active user to use in calculating predictions?


```{r Model-Based Algorithm}
train_func <= function(data, C = 1, tau = 0.05, iter = 100){ # function arguments include data, number of cluster, tau (threshold for termination when values converge), and number of iterations
  # using tau = 0.05 from previous year's code but no particular understanding
  data <- movie_UI[1:500, ] # subset for speed (until finished)
  C <- 12 # Half of the movie genres according to IMDB
  # Step 1 - Initialization 
  
  # Step 2 - Expectation
  
  # Step 3 - Maximization
  
}


```
Model-based recommendation intermediate steps:
-(E-step) Write code to give cluster assignment probabilities to user, a matrix where cols are clusters and rows are users. output = assignment probability matrix.
-(M- Step) Write code to re-estimate mu and gamma vectors from each users cluster assignment probabilities. There are $k*C*j$ gamma values which can be put into a 3d array or a series of matrices. Output = recalculated mu and gamma (matrix).
- Write code to iterate of the the two steps and to set when to stop iteration(consider iteration back stop number), and set up initial $\mu_c$ and $\gamma$ values.
-Write code to calculate expected active user rating from EM parameters. Output = function with input of data and # of classes and output of predicted rating on movie m for active user.
-Write CV code to determine best test error on predictions at different numbers of classes for the EM/predictor model. Include code to calculate test error (shared with memory-based team). Output = optimal number of classes. 
-Re calibrate for microsoft data.


Notes on EM: 
- Use cross-validation to find optimal number of clusters (if computationally taxing, then use a k-fold CV where k = 1)
- Tricky part of EM: when to terminate algorithm
- Can do random initializations and find log-likelihood but not necessary

Notes to make predictions from EM: 
- After the termination of the EM algorithm, give a hard assignment of clusters (EM algorithm only gives soft assignment so use the highest probability that a user falls within a specific cluster to create hard assignment)
- For each movie-cluster pair, you have a multinomial distribution (this is $\gamma$); use this probability distribution to calculate the rating that each user \textit{a} gave to movie \textit{m} within each cluster \textit{c}. 
- All of the users within each cluster have the same rating probability. 
